{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watsite On-the-fly\n",
    "\n",
    "\n",
    "\n",
    "This notebook contains the code for grid-to-grid mapping model as described in our publication. \n",
    "Training data files are not included, since they are producable by the commercial program FLAP. You can purchase FLAP to generate your own data, or use any other input type with similar properties. \n",
    "\n",
    "This model works with 48x48x48 grids, and arrays have to be in h5 format with the shape (48,48,48,number of channels). The output grid will be (48,48,48,1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version is: 2.2.4\n",
      "Tensorflow version is: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os,glob,time\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.layers import Activation, Input, Dropout, merge, Concatenate, multiply,concatenate,add\n",
    "from keras.layers.convolutional import Conv3D, UpSampling3D, Deconv3D,Conv3DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Reshape, Lambda,GaussianNoise,MaxPooling3D\n",
    "from keras.utils import generic_utils as keras_generic_utils\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import callbacks\n",
    "import matplotlib.pyplot\n",
    "\n",
    "\n",
    "\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "\n",
    "np.random.seed(42) # for consistent results\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']= 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "print('Keras version is:',keras.__version__)\n",
    "print('Tensorflow version is:',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models: Inception model and baseline U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block(inputs, depth, batch_mode=0, splitted=False, activation='relu'):\n",
    "    assert depth % 16 == 0\n",
    "    actv = activation == 'relu' and (lambda: LeakyReLU(0.0)) or activation == 'elu' and (lambda: ELU(1.0)) or None\n",
    "    \n",
    "    c1_1 = Conv3D(depth//4, 1, init='he_normal', padding='same')(inputs)\n",
    "    \n",
    "    c2_1 = Conv3D(depth//8*3, 1, init='he_normal', padding='same')(inputs)\n",
    "    c2_1 = actv()(c2_1)\n",
    "    if splitted:\n",
    "        print('WARNING: Splitted not supported.')\n",
    "        c2_2 = Conv3D(depth//2,(1, 3,1), init='he_normal', padding='same')(c2_1)\n",
    "        c2_2 = BatchNormalization(axis=-1)(c2_2)\n",
    "        c2_2 = actv()(c2_2)\n",
    "        c2_3 = Conv3D(depth//2, (3, 1,1), init='he_normal', padding='same')(c2_2)\n",
    "    else:\n",
    "        c2_3 = Conv3D(depth//2, 3, init='he_normal', padding='same')(c2_1)\n",
    "    \n",
    "    c3_1 = Conv3D(depth//16, 1, init='he_normal', padding='same')(inputs)\n",
    "    #missed batch norm\n",
    "    c3_1 = actv()(c3_1)\n",
    "    if splitted:\n",
    "        c3_2 = Conv3D(depth//8, 1, 5, init='he_normal', padding='same')(c3_1)\n",
    "        c3_2 = BatchNormalization(axis=-1)(c3_2)\n",
    "        c3_2 = actv()(c3_2)\n",
    "        c3_3 = Conv3D(depth//8, 5, 1, init='he_normal', padding='same')(c3_2)\n",
    "    else:\n",
    "        c3_3 = Conv3D(depth//8, 5, init='he_normal', padding='same')(c3_1)\n",
    "    \n",
    "    p4_1 = MaxPooling3D(pool_size=3, strides=1, padding='same')(inputs)\n",
    "    c4_2 = Conv3D(depth//8, 1, init='he_normal', padding='same')(p4_1)\n",
    "    \n",
    "    #res = merge([c1_1, c2_3, c3_3, c4_2], mode='concat', concat_axis=-1)\n",
    "    res = concatenate([c1_1, c2_3, c3_3, c4_2],axis=-1)\n",
    "    res = BatchNormalization(axis=-1)(res)\n",
    "    res = actv()(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "def _shortcut(_input, residual):\n",
    "    stride_width = _input._keras_shape[1] // residual._keras_shape[1]\n",
    "    stride_height = _input._keras_shape[2] // residual._keras_shape[2]\n",
    "    stride_depth = _input._keras_shape[3] // residual._keras_shape[3]\n",
    "    equal_channels = residual._keras_shape[4] == _input._keras_shape[4]\n",
    "\n",
    "    shortcut = _input\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or stride_depth > 1 or not equal_channels:\n",
    "        shortcut = Conv3D(residual._keras_shape[0], 1,\n",
    "                                 strides=(stride_width, stride_height,stride_depth),\n",
    "                                 init=\"he_normal\", padding=\"valid\")(_input)\n",
    "\n",
    "    #return merge([shortcut, residual], mode=\"sum\")\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "\n",
    "def rblock(inputs, num, depth, scale=0.1):    \n",
    "    residual = Conv3D(depth, num, padding='same')(inputs)\n",
    "    residual = BatchNormalization(axis=-1)(residual)\n",
    "    residual = Lambda(lambda x: x*scale)(residual)\n",
    "    res = _shortcut(inputs, residual)\n",
    "    return ELU()(res) \n",
    "\n",
    "def NConvolution3D(nb_filter, dim, padding='same', strides=1):\n",
    "    def f(_input):\n",
    "        conv = Conv3D(nb_filter, dim, strides=strides,\n",
    "                              padding=padding)(_input)\n",
    "        norm = BatchNormalization(axis=-1)(conv)\n",
    "        return ELU()(norm)\n",
    "\n",
    "    return f\n",
    "\n",
    "def inception(input_img_dim, num_output_channels):\n",
    "    optimizer = Adam(lr=0.045, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    splitted = False\n",
    "    act = 'elu'\n",
    "    \n",
    "    inputs = Input((input_img_dim), name='main_input')\n",
    "    conv1 = inception_block(inputs, 32, batch_mode=2, splitted=splitted, activation=act)\n",
    "    #conv1 = inception_block(conv1, 32, batch_mode=2, splitted=splitted, activation=act)\n",
    "    \n",
    "    #pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = NConvolution3D(32, 3, padding='same', strides=2)(conv1)\n",
    "    pool1 = Dropout(0.5)(pool1)\n",
    "    \n",
    "    conv2 = inception_block(pool1, 64, batch_mode=2, splitted=splitted, activation=act)\n",
    "    #pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = NConvolution3D(64, 3, padding='same', strides=2)(conv2)\n",
    "    pool2 = Dropout(0.5)(pool2)\n",
    "    \n",
    "    conv3 = inception_block(pool2, 128, batch_mode=2, splitted=splitted, activation=act)\n",
    "    #pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = NConvolution3D(128, 3, padding='same', strides=2)(conv3)\n",
    "    pool3 = Dropout(0.5)(pool3)\n",
    "     \n",
    "    conv4 = inception_block(pool3, 256, batch_mode=2, splitted=splitted, activation=act)\n",
    "    #pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = NConvolution3D(256, 3, padding='same', strides=2)(conv4)\n",
    "    pool4 = Dropout(0.5)(pool4)\n",
    "    \n",
    "    conv5 = inception_block(pool4, 512, batch_mode=2, splitted=splitted, activation=act)\n",
    "    #conv5 = inception_block(conv5, 512, batch_mode=2, splitted=splitted, activation=act)\n",
    "    conv5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "\n",
    "    \n",
    "    after_conv4 = rblock(conv4, 1, 256)\n",
    "\n",
    "    up6 = concatenate([UpSampling3D(size=2)(conv5), after_conv4],axis=-1)\n",
    "    conv6 = inception_block(up6, 256, batch_mode=2, splitted=splitted, activation=act)\n",
    "    conv6 = Dropout(0.5)(conv6)\n",
    "    \n",
    "    after_conv3 = rblock(conv3, 1, 128)\n",
    "    up7 = concatenate([UpSampling3D(size=2)(conv6), after_conv3], axis=-1)\n",
    "    conv7 = inception_block(up7, 128, batch_mode=2, splitted=splitted, activation=act)\n",
    "    conv7 = Dropout(0.5)(conv7)\n",
    "    \n",
    "    after_conv2 = rblock(conv2, 1, 64)\n",
    "    up8 = concatenate([UpSampling3D(size=2)(conv7), after_conv2], axis=-1)\n",
    "    conv8 = inception_block(up8, 64, batch_mode=2, splitted=splitted, activation=act)\n",
    "    conv8 = Dropout(0.5)(conv8)\n",
    "    \n",
    "    after_conv1 = rblock(conv1, 1, 32)\n",
    "    up9 = concatenate([UpSampling3D(size=2)(conv8), after_conv1], axis=-1)\n",
    "    conv9 = inception_block(up9, 32, batch_mode=2, splitted=splitted, activation=act)\n",
    "\n",
    "    conv9 = Dropout(0.5)(conv9)\n",
    "\n",
    "    conv10 = Conv3D(num_output_channels, 1, init='he_normal', activation='sigmoid', name='main_output')(conv9)\n",
    "\n",
    "\n",
    "    model = Model(input=inputs, output=[conv10])\n",
    "    model.compile(optimizer=optimizer,loss=gen_dice_loss,metrics=['acc',precision,recall])\n",
    "\n",
    "    return model    \n",
    "\n",
    "\n",
    "def baselineUnet(input_img_dim, num_output_channels):\n",
    "    stride = 2\n",
    "\n",
    "\n",
    "    # batch norm merge axis\n",
    "    bn_axis = -1\n",
    "\n",
    "    input_layer = Input(shape=input_img_dim, name=\"unet_input\")\n",
    "\n",
    "    # 1 encoder C64\n",
    "\n",
    "    en_1 = Conv3D(32, 2, padding='same', strides=stride)(input_layer)\n",
    "    en_1 = BatchNormalization(name='gen_en_bn_1',  axis=bn_axis)(en_1)\n",
    "    en_1 = LeakyReLU(alpha=0.2)(en_1)\n",
    "    \n",
    "    # 2 encoder C128\n",
    "    en_2 = Conv3D(64, 2, padding='same', strides=stride)(en_1)\n",
    "    en_2 = BatchNormalization(name='gen_en_bn_2',  axis=bn_axis)(en_2)\n",
    "    en_2 = Dropout(p=0.5)(en_2)\n",
    "    en_2 = LeakyReLU(alpha=0.2)(en_2)\n",
    "    \n",
    "    # 3 encoder C256\n",
    "    \n",
    "    en_3 = Conv3D(128, 2, padding='same', strides=stride)(en_2)\n",
    "    en_3 = BatchNormalization(name='gen_en_bn_3', axis=bn_axis)(en_3)\n",
    "    en_3 = Dropout(p=0.5)(en_3)\n",
    "    en_3 = LeakyReLU(alpha=0.2)(en_3)\n",
    "    \n",
    "    # 4 encoder C512\n",
    "    en_4 = Conv3D(256, 2, padding='same', strides=stride)(en_3)\n",
    "    en_4 = BatchNormalization(name='gen_en_bn_4', axis=bn_axis)(en_4)\n",
    "    en_4 = Dropout(p=0.5)(en_4)\n",
    "    en_4 = LeakyReLU(alpha=0.2)(en_4)\n",
    "    \n",
    "    # 5 encoder C512\n",
    "    en_5 = Conv3D(512, 2, padding='same', strides=stride)(en_4)\n",
    "    en_5 = BatchNormalization(name='gen_en_bn_5', axis=bn_axis)(en_5)\n",
    "    en_5 = Dropout(p=0.5)(en_5)\n",
    "    en_5 = LeakyReLU(alpha=0.2)(en_5)\n",
    "    \n",
    "    # 6 encoder C512\n",
    "    en_6 = Conv3D(512, 2,padding='same', strides=stride)(en_5)\n",
    "    en_6 = BatchNormalization(name='gen_en_bn_6', axis=bn_axis)(en_6)\n",
    "    en_6 = Dropout(p=0.5)(en_6)\n",
    "    en_6 = LeakyReLU(alpha=0.2)(en_6)\n",
    "    \n",
    "    # 4 decoder CD1024 (decodes en_5)\n",
    "    de_4 = UpSampling3D(size=3)(en_6)\n",
    "    de_4 = Conv3D(512, 2, padding='same')(de_4)\n",
    "    de_4 = BatchNormalization(name='gen_de_bn_4', axis=bn_axis)(de_4)\n",
    "    de_4 = Dropout(p=0.5)(de_4)\n",
    "    de_4 = concatenate([de_4, en_4],axis=-1)\n",
    "    de_4 = Activation('relu')(de_4)\n",
    "    \n",
    "    # 5 decoder CD1024 (decodes en_4)\n",
    "    de_5 = UpSampling3D(size=(2, 2, 2))(de_4) #short-circuit! should be de_4\n",
    "    de_5 = Conv3D(256, 2, padding='same')(de_5)\n",
    "    de_5 = BatchNormalization(name='gen_de_bn_5', axis=bn_axis)(de_5)\n",
    "    de_5 = Dropout(p=0.5)(de_5)\n",
    "    de_5 = concatenate([de_5, en_3],axis=-1)\n",
    "    de_5 = Activation('relu')(de_5)\n",
    "    \n",
    "    \n",
    "    # 6 decoder C512 (decodes en_3) \n",
    "    de_6 = UpSampling3D(size=(2, 2, 2))(de_5) #short-circuit! should be de_5\n",
    "    de_6 = Conv3D(128, 2, padding='same')(de_6)\n",
    "    de_6 = BatchNormalization(name='gen_de_bn_6', axis=bn_axis)(de_6)\n",
    "    de_6 = Dropout(p=0.5)(de_6)\n",
    "    de_6 = concatenate([de_6, en_2], axis=-1)\n",
    "    de_6 = Activation('relu')(de_6)\n",
    "    \n",
    "    # 7 decoder CD256 (decodes en_2)\n",
    "    de_7 = UpSampling3D(size=(2, 2, 2))(de_6)\n",
    "    de_7 = Conv3D(64, 2, padding='same')(de_7)\n",
    "    de_7 = BatchNormalization(name='gen_de_bn_7', axis=bn_axis)(de_7)\n",
    "    de_7 = Dropout(p=0.5)(de_7)\n",
    "    de_7 = concatenate([de_7, en_1],axis=-1)\n",
    "    de_7 = Activation('relu')(de_7)\n",
    "\n",
    "\n",
    "    de_8 = UpSampling3D(size=(2, 2, 2))(de_7)\n",
    "    de_8 = Conv3D(num_output_channels, 3, padding='same')(de_8)\n",
    "    de_8 = Activation('sigmoid')(de_8)\n",
    "\n",
    "    unet_generator = Model(input=[input_layer], output=[de_8], name='unet_generator')\n",
    "    return unet_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "\n",
    "    x = np.clip(x,-20.0,20.0)/10\n",
    "\n",
    "    return x\n",
    "    \n",
    "\n",
    "def normalize_wat(X,thresh):\n",
    "\n",
    "\n",
    "    X[np.where(X>thresh)]=1.\n",
    "\n",
    "\n",
    "    return X\n",
    "  \n",
    "thresh = [0.,0.02,0.03,0.045,0.06,0.07] # thresholds for water occupancy\n",
    "\n",
    "def load_grids(folders,batch_size=10,printname=False,shuffle=True,permutate_axis=None):\n",
    "    \n",
    "    \n",
    "    # iterate forever bc keras requires this\n",
    "    num_images = len(folders)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(folders)\n",
    "        for batch_num in range(0, num_images-1, batch_size):\n",
    "            i = batch_num\n",
    "            if num_images-i < batch_size:\n",
    "                i_end = num_images-1\n",
    "            else:\n",
    "                i_end = i + batch_size\n",
    "            #open h5 files\n",
    "            files = []\n",
    "            for j in range(i,i_end):\n",
    "                if printname:\n",
    "                    fil = open('dataset_list.txt','a')\n",
    "                    print('Folder: {0}'.format(folders[j]),file=fil)\n",
    "                    fil.close()\n",
    "                try:\n",
    "                    files.append(h5py.File(folders[j], 'r'))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "            x_batch_atomgrids =[]\n",
    "            y_batch_watergrids = []\n",
    "            for k in range(len(files)):\n",
    "\n",
    "                #this part is for eli5 lib feature importance\n",
    "                if permutate_axis is not None:\n",
    "                    assert(type(permutate_axis==type(1)))\n",
    "                    x_grids = files[k]['x'][:]\n",
    "                    assert(permutate_axis<x_grids.shape[-1])\n",
    "                    n = np.random.permutation(x_grids[...,permutate_axis])\n",
    "                    x_grids[...,permutate_axis] = n\n",
    "                    x_batch_atomgrids.append(x_grids)\n",
    "                        \n",
    "                else:        \n",
    "                    x_batch_atomgrids.append(files[k]['x'][:])\n",
    "                \n",
    "                all_y = []\n",
    "                y_batch = files[k]['y'][:,:,:,0]\n",
    "                for t in thresh:\n",
    "                    all_y.append(normalize_wat(np.array(y_batch),t))\n",
    "                \n",
    "                y_batch_watergrids.append(np.stack(all_y,axis=-1))\n",
    "\n",
    "                \n",
    "                files[k].close()\n",
    "\n",
    "            # slice the specific batch that we want and output it through the generator\n",
    "            \n",
    "            \n",
    "            \n",
    "            x_batch_atomgrids =np.nan_to_num(x_batch_atomgrids)\n",
    "            x_batch_atomgrids = normalize(np.array(x_batch_atomgrids))\n",
    "            y_batch_watergrids =np.nan_to_num(y_batch_watergrids)\n",
    "            #y_batch_watergrids = normalize_wat(np.array(y_batch_watergrids))\n",
    "            \n",
    "\n",
    "            yield x_batch_atomgrids, y_batch_watergrids\n",
    "            \n",
    "\n",
    "            \n",
    "def process_batch(j):\n",
    "        if printname:\n",
    "                print('Folder: {0}'.format(folders[j]))\n",
    "        try:\n",
    "            f=(h5py.File(j, 'r'))\n",
    "        except Exception as e:\n",
    "            return\n",
    "        \n",
    "        x_batch = f['x'][:]\n",
    "        y_batch = np.expand_dims(f['y'][:,:,:,0],axis=-1)\n",
    "        f.close()\n",
    "        return x_batch,y_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losses and metrics and other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "def cropped_dice(y_true,y_pred,edge=20):\n",
    "    #calculate dice < 5. A. gets 48x48x48 returns 20x20x20\n",
    "    start = 48//2 - edge//2\n",
    "    end = start+edge\n",
    "    \n",
    "    #return dice_coef(y_true[:,start:end,start:end,start:end,:], y_pred[:,start:end,start:end,start:end,:])\n",
    "    return recall(y_true[:,start:end,start:end,start:end,:], y_pred[:,start:end,start:end,start:end,:])\n",
    "\n",
    "\n",
    "\n",
    "def dice_coefprint(y_true, y_pred, smooth=K.epsilon()):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    dice = (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "    return tf.Print(dice,[dice])\n",
    "\n",
    "\n",
    "def dice_coef_thresh(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred) # y_pred_f = K.cast(K.greater(K.flatten(y_pred), Threshold), 'float32')\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=K.epsilon()):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "def gen_dice_loss(y_true, y_pred):\n",
    "    '''\n",
    "    computes the sum of two losses : generalised dice loss and weighted cross entropy\n",
    "    '''\n",
    "\n",
    "    #generalised dice score is calculated as in this paper : https://arxiv.org/pdf/1707.03237\n",
    "    y_true_f = y_true\n",
    "    y_pred_f = y_pred\n",
    "    sum_p=K.sum(y_pred_f)\n",
    "    sum_r=K.sum(y_true_f)\n",
    "    sum_pr=K.sum(multiply([y_true_f,y_pred_f]))\n",
    "    weights=K.pow(K.square(sum_r)+K.epsilon(),-1)\n",
    "    generalised_dice_numerator =2*K.sum(weights*sum_pr)\n",
    "    generalised_dice_denominator =K.sum(weights*(sum_r+sum_p))\n",
    "    generalised_dice_score =generalised_dice_numerator /generalised_dice_denominator\n",
    "    GDL=1-generalised_dice_score\n",
    "    del sum_p,sum_r,sum_pr,weights\n",
    "\n",
    "    return GDL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_gen(generator_model,validation_atomgrid,wat_ground_truth,epoch):\n",
    "    \n",
    "    watgrid = generator_model.predict(validation_atomgrid)\n",
    "    #watgrid = np.arctanh((watgrid+0.99999)/2)\n",
    "    watgrid *= 35 #np.arctanh(watgrid)*35\n",
    "    wat_ground_truth *= 35 #np.arctanh(wat_ground_truth)*35\n",
    "\n",
    "    np.save('e{0}_predicted.npy'.format(epoch),watgrid)\n",
    "    np.save('e{0}_groundtruth.npy'.format(epoch),wat_ground_truth)\n",
    "\n",
    "def feed_to_generator(gen_model,X,scaling_fac=1.0):\n",
    "    #returns fake y\n",
    "    fake_y = gen_model.predict(X)*scaling_fac\n",
    "    return fake_y\n",
    "    \n",
    "    \n",
    "def water_predictor(generator_model,atomgrid,wat,prefix=''):\n",
    "\n",
    "    \n",
    "    pr_wat = feed_to_generator(generator_nn,atomgrid)\n",
    "    print(pr_wat.shape)\n",
    "    pr_wat = np.sum(pr_wat,axis=-1)\n",
    "\n",
    "    \n",
    "    pr_wat =  np.reshape(pr_wat,(48,48,48))\n",
    "    wat = np.sum(wat,axis=-1)\n",
    "    wat = np.reshape(wat,(48,48,48))\n",
    "    \n",
    "    np.save(prefix+'watsitemapper_predicted.npy',pr_wat)\n",
    "    np.save(prefix+'watsitemapper_groundtruth.npy',wat)\n",
    "    \n",
    "def water_predictor_num(generator_model,atomgrid,wat,i,prefix=''):\n",
    "\n",
    "    \n",
    "    pr_wat = feed_to_generator(generator_nn,atomgrid)\n",
    "\n",
    "    pr_wat = np.sum(pr_wat,axis=-1)\n",
    "\n",
    "    \n",
    "    pr_wat =  np.reshape(pr_wat,(48,48,48))\n",
    "    wat = np.sum(wat,axis=-1)\n",
    "    wat = np.reshape(wat,(48,48,48))\n",
    "\n",
    "\n",
    "    #wat = np.reshape(wat,wat.shape[1:-1])\n",
    "    \n",
    "    np.save(prefix+'watsitemapper_predicted_{0}.npy'.format(i),pr_wat)\n",
    "    np.save(prefix+'watsitemapper_groundtruth_{0}.npy'.format(i),wat)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_precision(y_true,y_pred):\n",
    "    auc = tf.metrics.precision(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "def tf_recall(y_true,y_pred):\n",
    "    auc = tf.metrics.recall(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compile and run\n",
    "\n",
    "Two pre-trained models are provided, the Inception U-Net model and the Baseline U-Net. To use either, approprite model creator function should be called and its corresponding weights file should be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probes_idx = [22, 53, 30, 47, 66, 24, 4, 58, 33, 61, 19, 32]\n",
    "\n",
    "\n",
    "\n",
    "#save_weight = callbacks.ModelCheckpoint('weights/weights.inception.mapper_watsite_multithresh_koesset.hdf5', monitor='val_loss', verbose=0, save_best_only=True, period=1)\n",
    "#save_weight = callbacks.ModelCheckpoint('weights/weights.unetbaseline.mapper_watsite_multithresh_koesset.hdf5', monitor='val_loss', verbose=0, save_best_only=True, period=1)\n",
    "\n",
    "im_width = im_height = im_depth = 48\n",
    "# inpu/oputputt channels in image\n",
    "input_channels = 12\n",
    "output_channels = len(thresh) #the thresholds for water occupancy\n",
    "input_img_dim = (im_width, im_height, im_depth,input_channels) #channel should be at the end\n",
    "output_img_dim = (im_width, im_height, im_depth,output_channels)\n",
    "#generator_nn = inception(input_img_dim, output_channels) #inception unet\n",
    "generator_nn  = baselineUnet(input_img_dim, output_channels) #baseline unet\n",
    "\n",
    "#generator_nn.summary()\n",
    "\n",
    "opt = Adam(lr=1E-3) #, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "\n",
    "generator_nn.load_weights('weights/weights.unetbaseline.mapper_watsite_multithresh_koesset.hdf5')\n",
    "\n",
    "generator_nn.compile(loss=gen_dice_loss, optimizer=opt,metrics=['mae','acc',precision,recall,dice_coef,cropped_dice])\n",
    "batch_size = 16\n",
    "\n",
    "#data_path = '..' # change this\n",
    "#datafiles = glob.glob(data_path+'/*/newwatsite_grids.h5')\n",
    "\n",
    "def get_list(f):\n",
    "    with open(f,'r') as fin:\n",
    "        pdbs = fin.read().splitlines()\n",
    "    pdbs = [data_path+'/'+i+'/newwatsite_grids.h5' for i in pdbs]\n",
    "    return pdbs\n",
    "\n",
    "#trainlist = get_list('../train_test_sets/all_0.5_2__reducedtrain0.list')\n",
    "#testlist = get_list('../train_test_sets/all_0.5_2__reducedtest0.list')\n",
    "\n",
    "#datafiles = check_converged(datafiles)\n",
    "#trainfiles, valfiles = train_test_split(datafiles,test_size=.1)\n",
    "#trainfiles = np.array(datafiles[100:])\n",
    "#valfiles = np.array(datafiles[0:100])\n",
    "#trainfiles = np.array(list(set(datafiles).intersection(set(trainlist))))\n",
    "#valfiles = np.array(list(set(datafiles).intersection(set(testlist))))\n",
    "\n",
    "#np.random.shuffle(trainfiles)\n",
    "#np.random.shuffle(valfiles)\n",
    "\n",
    "#trainfiles = trainfiles[0:7000]\n",
    "\n",
    "#print('trains:',len(trainfiles))\n",
    "#print('tests:',len(valfiles))\n",
    "#nb_epoch = 1001\n",
    "#n_images_per_epoch = len(trainfiles)\n",
    "#tng_gen = load_grids(trainfiles,batch_size=batch_size)\n",
    "#val_gen = load_grids(valfiles,batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "#generator_nn.fit_generator(tng_gen, epochs=nb_epoch,steps_per_epoch=len(trainfiles)/batch_size, verbose=2, validation_data=val_gen,validation_steps=len(valfiles)/batch_size, shuffle=True,callbacks=[save_weight]) \n",
    "#out  = generator_nn.evaluate_generator(val_gen,steps=len(valfiles)/batch_size,verbose=1) # to only evaluate, use this line\n",
    "#print(out) # get the loss and metrics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a prediction task\n",
    "\n",
    "The input is an hdf5 file, with two keys: 'x', which is the FLAP atom grids and 'y' which is water occupancy grids. The snippet below takes the grid file for pdb 1adl and generates two files, ground truth and predicted water grids and saves it as npy arrays. Npy arrays may later be converted to DX files for python visualization or other downstream tasks (such as input for Gnina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell to predict grids and save them as npy files\n",
    "\n",
    "\n",
    "data_path = '.'\n",
    "pdbs = ['1adl']\n",
    "grids = [data_path+'/'+i+'/newwatsite_grids.h5' for i in pdbs]\n",
    "\n",
    "grids = h5py.File(grids[0], 'r')\n",
    "\n",
    "X= np.array([grids['x'][:]])\n",
    "X=np.nan_to_num(X)\n",
    "#print(X.shape)\n",
    "X= normalize(X)             \n",
    "\n",
    "y = np.array([grids['y'][:,:,:,0]])\n",
    "y=np.nan_to_num(y)\n",
    "#print(y.shape)\n",
    "all_y = []\n",
    "\n",
    "for t in thresh:\n",
    "    all_y.append(normalize_wat(np.array(y),t))               \n",
    "\n",
    "    \n",
    "y= np.stack(all_y,axis=-1)\n",
    "grids.close()\n",
    "\n",
    "\n",
    "water_predictor_num(generator_nn,X,y,i,prefix='')\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
